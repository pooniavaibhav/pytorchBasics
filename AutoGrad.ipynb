{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoGrad.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPdn33/4hum+h662vZf9zYk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pooniavaibhav/pytorchBasics/blob/main/AutoGrad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAQx3RwE7cPd"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAQa3-WQ6-zJ",
        "outputId": "c6e84c5a-8fdd-4914-ad88-10b587ed7719"
      },
      "source": [
        "#Remember  if requires_grad=True, the tensor object keeps track of how it was created.\n",
        "x = torch.tensor([1.,2.,3], requires_grad=True)\n",
        "y = torch.tensor([4.,5.,6], requires_grad=True)\n",
        "# Notice that x and y have their required_grad set to true, therefore we can compute gradient w.r.t them-\n",
        "z = x + y\n",
        "print(z)\n",
        "# z knows that it was created as a result of addition of x and y and not just read from a file.\n",
        "print(z.grad_fn)\n",
        "s = z.sum()\n",
        "print(s)\n",
        "print(s.grad_fn) "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5., 7., 9.], grad_fn=<AddBackward0>)\n",
            "<AddBackward0 object at 0x7f2d2cba3890>\n",
            "tensor(21., grad_fn=<SumBackward0>)\n",
            "<SumBackward0 object at 0x7f2d290d6990>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmpHfOlm9dwK",
        "outputId": "9d42034d-280a-4d35-ba9c-95091ff033f2"
      },
      "source": [
        "# now we back propogate on s, we can find the gradients of s wrt x-\n",
        "s.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MrNZ9iE-PJo",
        "outputId": "ea45d7c7-1844-40fa-f04f-3e676a9fd46b"
      },
      "source": [
        "# By default , Tensors have  'requires_grad' = False\n",
        "x = torch.randn(2,2)\n",
        "y = torch.randn(2,2)\n",
        "print(x.requires_grad, y.requires_grad)\n",
        "z = x+y\n",
        "#So you can not back propagate through z\n",
        "print(z.grad_fn)\n",
        "#Another way to set the required_grad = True is\n",
        "x.requires_grad_()\n",
        "y.requires_grad_()\n",
        "x = x + y\n",
        "print(z.grad_fn)\n",
        "print(z.requires_grad)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False False\n",
            "None\n",
            "None\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hE8ppNbAtJd",
        "outputId": "cf14542c-fc25-43ab-903c-104e39005e57"
      },
      "source": [
        "# z.detach() returns a tensor that shares the same storage as 'z' but with the computation history forgotten.\n",
        "#It does not know any thing about how it was computed. In other words, we have broken the Tensor away from \n",
        "# his past history.\n",
        "new_z = z.detach()\n",
        "print(new_z.grad_fn)\n",
        "\n",
        "# you can also stop autograd from tracking history on Tensor.This concept is helpful when applying transfer learning-\n",
        "print(x.requires_grad)\n",
        "print((x+10).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "  print((x+10).requires_grad)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUtbw085CcGC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}